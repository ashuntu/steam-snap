#!/usr/bin/python3

import os
import re

NVIDIA_ENV = (
    "__NV_PRIME_RENDER_OFFLOAD=1 "
    "__GLX_VENDOR_LIBRARY_NAME=nvidia "
    "__VK_LAYER_NV_optimus=NVIDIA_only"
)

def main():
    lspci = os.popen("lspci | grep VGA").read()
    gpus = {}
    env_string = ""

    for gpu in lspci.splitlines():
        split = gpu.split(" VGA compatible controller: ")
        pci = f"pci-0000_{split[0]}"\
            .replace(":", "_")\
            .replace(".", "_")\

        if "nvidia" in split[1].lower():
            gpus[int(re.sub(r"[^0-9]+", "", pci))] = f"DRI_PRIME={pci} {NVIDIA_ENV}"
        else:
            gpus[int(re.sub(r"[^0-9]+", "", pci))] = f"DRI_PRIME={pci}"

    env_string = gpus[max(list(gpus.keys()))]
    print(env_string)


if __name__ == "__main__":
    main()
